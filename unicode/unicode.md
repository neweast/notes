## 当我们讨论Unicode时，我们在讨论什么
### 字符集和字符集编码
在开始讨论之前，首先需要区分两个概念：

1. `字符集`
2. `字符集编码`

计算机是由美国人发明的，一开始用来计算弹道。后来，人们开始使用计算机来存储、传递信息。但是，我们都知道，计算机内部使用的是二进制，这也就意味着，要使用计算机来存储信息，势必需要将信息转换为二进制的形式。

那么，如何将信息转换为二进制的形式呢？

1. 首先，信息有很多种类型。比如：文字、声音、视频等等。在这里，我们只讨论最简单的形式 - 文字。
2. 每种文字都由各种各样不同的字符构成。构成某一种文字的所有字符的集合，我们把它称为这种文字的`字符集`。
3. 为了将文字转换为二进制的形式，首先要解决的，就是将字符转换为二进制的形式。将字符转换为二进制形式的过程，我们把它称为`字符集编码`。

### 为什么 1 byte = 8 bits
前文已经提到过，信息在计算机中以二进制的形式存储，二进制的基本单位是 `bit`。`byte` 是计算机里数据存储的最小单元，`1 byte = 8 bits`。那么，为什么 `1 byte` 不是 5、6 或者 7 `bits`，而要是 `8 bits` 呢？

1. 美国人的文字是英文，主要包括26个英文字母。关于如何用二进制来表示英文，会在后面介绍，现在只需要知道，计算机会将英文字符编码为二进制形式。

2. 相当早之前，有一种波多码，广泛用于电报通信。波多码使用 `5 bits`，但是很快人们就碰到了瓶颈，`5 bits` 最多只能表示32个字符，因此波多码仅仅能够支持大写字母，和少数标点符号，不能支持小写字母和数字。

    ![baud_tastiera_jpeg](http://img2.tbcdn.cn/L1/461/1/234d8a1ff5ea0797d32e092ce29dcf28ca446cdd)

3. 波多码之后，有很多机器开始使用 `6 bits` 来表示字符。但是这样子还是不够，`6 bits` 仅能表示 2 ^ 6 = 64 种字符，其中大写字母 26 个，小写字母 26 个，数字 10 个，除去这些，就只能表示 2 个标点符号了。

4. `6 bits` 不够，那就再增加 `1 bit`，于是就有了 `ASCII` 编码标准。在很长一段时间里，`ASCII` 编码标准对绝大多数用户来讲都是够用的。

5. `ASCII` 需要 `7 bits`，那么第 `8 bit` 为什么引入呢？首先，第 `8 bit` 作为奇偶校验位；其次，我们知道 CPU 数据总线的宽度都是2的幂次方，8080处理器使用的就是 `8 bits` 的结构，出于数据对齐的考虑，`8 bits` 比 `7 bits` 更好；另外，存储的成本一直在降低，“浪费”一个 `bit`，也能够接受。在这些因素之下，数据存储的最小单元为 `8 bits`，也就顺理成章了。

### ASCII 编码标准
前文已经提到过，`ASCII` 使用 `7 bits` 的二进制位来表示字符，那么，它是如何表示的呢？

其实挺简单，就是给每个字符编一个号，简称为编号。编号是一个数字，不同字符的编号不能重复，然后把这个数字转换为二进制形式。

编号 + 转换为二进制形式的过程，就是字符集编码的过程。

`ASCII` 编码标准里，每个字符对应了一个小于 128 的十进制数，所以每个字符都能够存放在一个 `byte` 的 `bits` 空间里。

![242210073648925](http://img1.tbcdn.cn/L1/461/1/2c91e7eae9720cdeeb4cb0fba10cc4d5968d3fe1)

    ASCII 的诞生，也有其历史背景。在 IBM 生产出第一台个人电脑之前，各个计算机设备制造商都有各自不同的编码标准，光 IBM 一家公司的不同产品中，就包括了 9 种不同的编码标准。后来出于不同计算机之间，数据传输的需要，人们不得不建立一种统一的编码标准，并将其命名为 ASCII。因此，我们可以总结下，标准的诞生，是因为不同计算机之前需要传输数据。为什么要强调这一点呢？因为历史总是惊人的相似，后面我们还会遇到同样的问题。

### 字符集编码标准的发展以及问题
一开始，`ASCII` 只使用了 `7 bits`，第 `8 bit` 用来作为奇偶校验位。后来欧洲各国因为字符数量不够用，扩展了 `ASCII` 标准，将第 `8 bit` 也存储了字符。 
![ascii](http://img2.tbcdn.cn/L1/461/1/c005d33d235784011b583e72d40483f08b726ad4)

与此同时，还有很多非英语语系的国家，压根就没办法使用 `ASCII` 标准。更有甚至，如中国、日本、韩国等使用汉字或其变体的国家，字符数量远远超过 256 个，纷纷制定了自己的字符集编码标准。

逐渐的，问题来了......

随着英特网的普及，不同国家的互联网用户，可以通过网络进行沟通。沟通的本质，其实就是计算机之间数据的传输。然后，这个时候呢，出现了和 `ASCII` 标准诞生之前同样的问题：各个国家使用的编码标准不一样，互相之间完全没办法进行沟通。

    我们先简单解释一下，为什么没办法沟通？假设，A是一个美国的计算机用户，B是一个中国的计算机用户。B本身是懂英文的，也就是说，A和B之间并没有英文沟通的障碍。然后呢，有一天，A向B发了一封邮件，内容是著名的“hello world！”。邮件通过互联网，传送到了B的电脑。但是，B的电脑使用的编码标准不是 ASCII，同时也不兼容 ASCII 标准，那么，B看到的内容可能就是“？？？？”，B就会说，完全看不懂你在说什么，但是A也表示很无辜啊。

那么，这个时候怎么办呢？可能聪明的你已经想到了，那我们就像制定`ASCII`标准一样，重新制定一个新的标准呗。
    
    这里，还需要先阐明一个之前提到过的内容，“计算机之间互相传输数据”。继续A和B之间的话题，A和B之间要进行通信，并不是说，B的计算机需要将A的邮件内容直接翻译成中文，而是要求B的计算机能够正确显示A邮件的英文内容。我们之前不是说了么，B是懂英文滴，至于不懂英文怎么办？很简单，找翻译呗。能够正确显示信息的内容，这才是我们制定新标准的目的所在。

能够正确显示信息的内容，是我们制定新标准的目的所在。

### unicode编码标准
人们开始讨论，新的标准一定要满足三个主要目标：

    1. universal 通用性（能够编码世界上的所有语言，其实就是囊括世界上所有语言的字符）
    2. uniform 一致性（为了提高访问效率，新的标准采用固定宽度编码）
    3. unique 唯一性（bit流到字符流只有唯一解释）

这三个主要目标，也正是`unicode`命名的由来。uni - code。

然后，在前文中，我们已经将计算机的基本存储单元定义为了 `byte ( 8 bits )`，`1 byte` 肯定是没办法编码世界上所有字符的，`ASCII` 标准就使用了 `1 byte`，最多只能编码 256 个不同的字符。`1 byte` 不够，那么`2 byte` 呢？`2 byte` 能够容纳 65536 个字符。（注意，因为已经将 `byte` 定义为了基本存储单元，至于原因，前文已经解释过了，那么我们就不可能增加 `4 bits` 或者 `6 bits` 之类的，肯定是增加 `byte` 的倍数。）针对这个问题，当时提出 `unicode` 编码计划的几个工程师做了初步的统计，统计结果是没有问题，完全可以囊括所有的字符。现在来看，不知道他们当时是如何统计的，反正事实是字符数量已经超过 65536，当然，我们不能否定他们工作的伟大。

`unicode`编码标准，因为使用两个 `byte`，简称为 `UCS-2`。

我不知道大家有没有注意到，`unicode`的三个主要目标里面，有提到希望使用固定宽度编码。为什么呢？

    和固定宽度编码对应的，是变长宽度编码。但是相比较而言，固定宽度编码会简单很多。举个列子，charAt，在所有字符串操作中，应该算使用频率很高的操作之一。如果使用固定宽度编码，每个字符都使用 2 byte 存储，那么字符串查找实现起来很简单，2 byte * index 就可以了。但是，如果使用变长编码，某些字符由 1 byte 存储，某些字符由 2 byte 存储，那查找算法实现起来就会复杂很多，因为查找的时候，得区分是 1 byte 还是 2 byte，查找的效率也会下降。与此同时，还有一个更加麻烦的问题，就是 2 byte 中可能会包含 1 byte 的内容，要保证 bit 流到字符流的唯一性，也会很麻烦。

一切新事物，都没那么快被人接受，`unicode`编码标准也一样。

    我们之前提到过，unicode 标准，对所有的字符，都使用 2 byte 进行二进制编码。但是，像美国这样，使用英语语系的国家，就不怎么乐意。对他们而言，ASCII 标准完全够用，而且只需要 1 byte，如果用 unicode 标准，文本文件就会凭空增大1倍的存储空间。

### UTF-8
在`unicode`之前，所有的编码标准，都只对应了一种编码方式。比如，`ASCII 标准`其实就是`ASCII 编码方式`，而且，并没有明确的区分编码标准和编码方式的概念。最初的`unicode标准`也只是简称为`UCS-2(unicode character set - 2 byte)`。

    编码标准和编码方式有什么区别呢？编码标准定义了字符集里面，每个字符的编号；编码方式决定了这个字符编号对应的二进制形式，其实就是二进制数。ASCII标准里面，字符、编号、二进制数都是一一对应的。

`unicode`来势汹汹，而且好处显而易见，像美国这样的英语语系国家，不可能一直弃之不用。但是，他们又不希望浪费存储空间，为了解决这个问题，有人提出了编码方式的概念。首先，在制定`unicode 标准`的时候，就做了 `ASCII  标准`的兼容，具体做法就是字符编号在256以下的`unicode码点`，完全和`ASCII 标准`一致。然后，在将这部分字符编码为二进制形式的时候，因为码点小于256，可以只使用一个字节。然后其他的码点超过256的字符，就使用2个，或者更多的字节。这种编码方式，取名为`UTF-8`，它是一种变长编码。

编码标准和编码方式分开之后，也就意味着字符集里的同一个字符，可能会对应多个不同的二进制数。同时，也意味着编码标准不用像以前一样，考虑每个字符会占用多少 `byte` 的情况，这些都交给了编码方式去考虑。

### Java
`Java` 语言的设计者，一开始就将内置字符串的编码标准设置为了 `unicode`。并且，考虑到固定宽度的编码方式，对字符串操作更加友好，于是在编码方式上，使用了UCS-2（每个字符都会被编码成`2 byte`），这也是为什么`char`是`16 bits`的原因。也就是说，`Java`语言，一开始就是支持国际化的，而且编码方式是固定长度编码。

### 问题又出现了
很长一段时间内，所有的一切工作的都很好。但是后来，不幸还是发生了：`unicode 编码标准`的65536个码点被发现不够使用了。

对于`unicode 编码标准`来说，处理方式很简单，因为`unicode 编码标准`和`unicode 编码方式`已经互相独立。65536个码点不够用，那就继续扩大码点数量，不用像以前一样，还需要考虑每个字符占用多少`byte`，对字符编码后的二进制进行扩容等等。这些工作都转嫁给了编码方式。

这个扩容，对`unicode 编码标准`来说很简单，但是对像`Java`这样的语言而言，因为使用了`UCS-2`编码方式，就麻烦了......

    以前 2 byte 就可以表示一个字符，但是现在不行了。以前的char类型、字符串处理函数什么的，可能都需要修改。

### Java如何解决这个问题
一开始，`unicode码点`的表示形式为`U+XXXX`，X代表一个十六制数字，长度为4位，不足4位时，在前面补0，直到补足4位。那个时候，码点的范围是U+0000~U+FFFF。

`unicode码点`数量超过65536后，码点的表示形式修改为了`U+[XX]XXXX`，X代表一个十六制数字，可以有4-6位，不足4位时，在前面补0，直到补足4位。超过4位后，该是几位就是几位。这样子之后，码点的范围变成了U+0000~U+10FFFF，码点数量的理论大小为`10FFFF+1=110000`（16进制），也就是 `（16 +1）* 65536`，一百万左右。为了更好分类管理如此庞大的码点数，把每65536个码点作为一个平面，总共17个平面。

![05010132_VZge](http://img4.tbcdn.cn/L1/461/1/24c09cc5f7d6f93f274ef5b118fadff3ad7258e2)

第一个平面即是`BMP`（Basic Multilingual Plane 基本多语言平面），也叫Plane 0，它的码点范围是`U+0000~U+FFFF`。这也是我们最常用的平面，日常用到的字符绝大多数都落在这个平面内。

后续的16个平面称为`SP`（Supplementary Planes）。显然，这些码点已经是超过`U+FFFF`的了，所以已经超过了16位空间的理论上限。

一开始，`Java`使用的`UCS-2`编码标准只能容纳65536个码点，为了容纳更多的码点，`Java`改用了`UCS-2`的升级版本`UTF-16`。`UTF-16`是一种变长的2或4字节编码模式。对于`BMP`内的字符使用2字节编码，其它的则使用4字节组成所谓的代理对来编码。`UTF-16`对于`BMP`内字符的二进制编码结果基本和`UCS-2`一致。

在`BMP`里，有一块区域被称为代理区。代理区是`UTF-16`为了编码增补平面中的字符而保留的，总共有2048个位置，均分为高代理区`（D800–DBFF）`和低代理区`（DC00–DFFF）`两部分，各1024，这两个区组成一个二维的表格，共有1024×1024=210×210=24×216=16×65536，所以它恰好可以表示增补的16个平面中的所有字符。

![05005224_ngsJ](http://img3.tbcdn.cn/L1/461/1/e7df7e9f5c9722e8193789cbfb9bfccf78b34d5e)

在`BMP`里，`UTF-16`采用`16 bits`来表示一个字符，被称为码点单元，`SP`里，`UTF-16`采用一对码点单元表示字符。我们可以简单的把`char`看成一个代码单元，也就是说有些字符需要1个`char`，有些需要2个`char`。

另外，还需要注意一点，这里提到的`UTF-16`编码方式，是指`Java`语言里`String`在内存中以是`UTF-16`方式编码的。

`Java`使用`UTF-16`之后，字符串操作方法也有了变化。举例来说，length()方法，返回的是代码单元的数量，而不是字符的数量，大部分常用字符都是一个代码单元，少数字符是两个代码单元。当我们在做字符串长度校验的时候，可能需要注意下这个地方。

![length](http://img4.tbcdn.cn/L1/461/1/aadd9edc925da327b0fda5e01289191b160f1d6b)

### 代码单元和UTF-8 UTF-16 UTF-32
前面引出了代码单元，那么代码单元到底是什么呢？一种转换格式（UTF）中最小的一个分隔，就被称为一个代码单元（Code Unit）。同时，一种转换格式只会包含整数个单元。

    UTF-8中的8指的就是8 bits为一个单元，也即一字节为一个单元，UTF-8可以包含一个单元，二个单元，三个单元及四个单元，对应即是一，二，三及四字节。

    UTF-16中的16指的就是16 bits为一个单元，也即二字节为一个单元，UTF-16可以包含一个单元和二个单元，对应即是二个字节和四个字节。我们操作UTF-16时就是以它的一个单元为基本单位的。

    同理，UTF-32以32 bits为一个单元，它只包含这一种单元就够了，它的一单元自然也就是四字节了。

总结下，`UTF-X`中的数字`X`就是各自代码单元的`bit`数。

### 总结
当不清楚事实来源的时候，我们可能会觉得不够踏实，似乎一切建立在无源之水，无本之木之上一样。说了很多，主要是一直对unicode的发展过程比较好奇，也希望能够给大家一些参考。